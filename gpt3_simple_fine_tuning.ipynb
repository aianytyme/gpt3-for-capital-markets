{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-3 For Capital Markets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6Bd5T/y2sMwRqMqDg0QET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aianytyme/gpt3-for-capital-markets/blob/main/gpt3_simple_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning GPT-3 For Capital Markets**\n",
        "\n",
        "References:\n",
        "\n",
        "1.   [Fine-tuning GPT3 model for github repo data](https://www.youtube.com/watch?v=Cf8m3Bflfuc)\n",
        "2.   [Analyzing SEC filings with Transformers for fun and profit](https://www.youtube.com/watch?v=SU1L6f0N6iw&list=PLcSRBAICQBoo0k7FR6MPAJbSWGx0WPyOI)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "88VhjMjG4Q6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial Playground"
      ],
      "metadata": {
        "id": "6hryRroA4pZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([(1,2,3), (4,5,6)])\n",
        "c = np.array([[1], [2], [3]])\n",
        "\n",
        "print ('a: dim:{}, shape:{}'.format(a.ndim, a.shape))\n",
        "print ('b: dim:{}, shape:{}'.format(b.ndim, b.shape))\n",
        "print ('c: dim:{}, shape:{}'.format(c.ndim, c.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YBZnoij3uBf",
        "outputId": "aaee933c-f658-4819-db5e-73f94978ea60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: dim:1, shape:(3,)\n",
            "b: dim:2, shape:(2, 3)\n",
            "c: dim:2, shape:(3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "EgL07aG6IiS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Libraries**"
      ],
      "metadata": {
        "id": "v7Blys0sIhkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyGithub\n",
        "!pip install python-dotenv\n",
        "!pip install --upgrade jsonlines\n",
        "!pip install --upgrade openai\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "rS0QIr9NeB09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3483b74-96e5-4a0e-b71c-2ffbbfc7e393"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyGithub\n",
            "  Downloading PyGithub-1.55-py3-none-any.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pynacl>=1.4.0\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.7/dist-packages (from PyGithub) (2.23.0)\n",
            "Collecting pyjwt>=2.0\n",
            "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pynacl>=1.4.0->PyGithub) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.14.0->PyGithub) (2021.10.8)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->PyGithub) (1.14.0)\n",
            "Installing collected packages: pynacl, pyjwt, deprecated, PyGithub\n",
            "Successfully installed PyGithub-1.55 deprecated-1.2.13 pyjwt-2.3.0 pynacl-1.5.0\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.20.0\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (4.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.0.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.18.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 804 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.58-py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.18.1-py3-none-any.whl size=53168 sha256=8c6200ba719221cd8ab1a23b613c0b46c5f6b78142bdcea72786b5e9838e4d9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/bf/24/fcdc9d2b81f9c7e565bb2036ec9f7cc930056b829895b3bf48\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, openai\n",
            "Successfully installed openai-0.18.1 pandas-stubs-1.2.0.58\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 27.5 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=7c6b459a6c15ea557fc663c697ba09432bc407e0b0022b6855372e119506f90b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Drive**"
      ],
      "metadata": {
        "id": "NM9SKlLsLkhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "PATH= r'/content/drive/MyDrive/Projects/GPT-3/'\n",
        "print(\"Path:{}\".format(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-NWfQj9Lu-Q",
        "outputId": "fe39aeab-ea6d-4b4b-9fd1-c6744ab028fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Path:/content/drive/MyDrive/Projects/GPT-3/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Environment Variables**\n",
        "\n",
        "**Plese note this jupyter notebooks expects environment variables to be in .env file in Google Drive**\n",
        "* Location : /content/drive/MyDrive/Projects/GPT-3/ )\n",
        "* Contents :\n",
        "<p>FLASK_APP=app\n",
        "<p>FLASK_ENV=development\n",
        "<p>OPENAI_API_KEY=[YOUR OPEN_API_KEY]\n",
        "<p>GITHUB_ACCESS_TOKEN=[AIANYTYME GITHUB ACCESS KEY]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DJRIFuCMuvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(PATH+'.env')\n",
        "print(\"Env File to load:{}\".format(PATH+'.env'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xilzKAHMrnE",
        "outputId": "74e138fc-c570-4dc9-f710-b7986ab32584"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env File to load:/content/drive/MyDrive/Projects/GPT-3/.env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "c8oxcWgdZrmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github\n",
        "import os\n",
        "import jsonlines"
      ],
      "metadata": {
        "id": "dgjaU6IwZlH9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data from Github Repositories"
      ],
      "metadata": {
        "id": "Co7CGGZnfCqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getenv(\"GITHUB_ACCESS_TOKEN\"))\n",
        "g = Github(os.getenv(os.getenv(\"GITHUB_ACCESS_TOKEN\")))\n",
        "repos = g.get_organization(\"aianytyme\").get_repos()"
      ],
      "metadata": {
        "id": "wGsxeLTZfP4f",
        "outputId": "d5908216-6ee4-486b-f654-1051670b2aec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jsons = []\n",
        "for repo in repos:\n",
        "  try:\n",
        "    print(\"repo.full_name:{}\".format(repo.full_name))\n",
        "    file_content = repo.get_contents(\"README.md\")\n",
        "    jsons.append({\"prompt\": repo.full_name, \"completion\": file_content.decoded_content.decode()})\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "print(len(jsons))\n",
        "for j in jsons:\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "NC3rPByPftP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77485f9e-31c7-4cf6-83b6-c65b8c88109b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repo.full_name:aianytyme/openai-quickstart-python\n",
            "repo.full_name:aianytyme/DS-Tutorials-Notebooks\n",
            "repo.full_name:aianytyme/gpt3-for-capital-markets\n",
            "repo.full_name:aianytyme/gpt-3\n",
            "repo.full_name:aianytyme/sagemaker-huggingface-inference-toolkit\n",
            "repo.full_name:aianytyme/NLCA_Question_Generator\n",
            "6\n",
            "{'prompt': 'aianytyme/openai-quickstart-python', 'completion': '# OpenAI API Quickstart - Python example app\\n\\nThis is an example pet name generator app used in the OpenAI API [quickstart tutorial](https://beta.openai.com/docs/quickstart). It uses the [Flask](https://flask.palletsprojects.com/en/2.0.x/) web framework. Check out the tutorial or follow the instructions below to get set up.\\n\\n## Setup\\n\\n1. If you don’t have Python installed, [install it from here](https://www.python.org/downloads/)\\n\\n2. Clone this repository\\n\\n3. Navigate into the project directory\\n\\n   ```bash\\n   $ cd openai-quickstart-python\\n   ```\\n\\n4. Create a new virtual environment\\n\\n   ```bash\\n   $ python -m venv venv\\n   $ . venv/bin/activate\\n   ```\\n\\n5. Install the requirements\\n\\n   ```bash\\n   $ pip install -r requirements.txt\\n   ```\\n\\n6. Make a copy of the example environment variables file\\n\\n   ```bash\\n   $ cp .env.example .env\\n   ```\\n\\n7. Add your [API key](https://beta.openai.com/account/api-keys) to the newly created `.env` file\\n\\n8. Run the app\\n\\n   ```bash\\n   $ flask run\\n   ```\\n\\nYou should now be able to access the app at [http://localhost:5000](http://localhost:5000)! For the full context behind this example app, check out the [tutorial](https://beta.openai.com/docs/quickstart).\\n'}\n",
            "{'prompt': 'aianytyme/DS-Tutorials-Notebooks', 'completion': '# DS-Tutorials-Notebooks\\nNotebooks from the Data Science tutorials\\n'}\n",
            "{'prompt': 'aianytyme/gpt3-for-capital-markets', 'completion': \"## GPT-3 for Capital markets Notebook \\nNext Steps\\n* Replicate steps in your own colab environment \\n* Familiarize yourself with connections to Github, Google Drive and possible Kaggle\\n* Potential Financial Data to train on \\n  * Use 10 K data - The SEC mandates that all public companies file regular 10-Ks to keep investors aware of a company's financial condition and to allow them to have enough information before they buy or sell securities issued by that company. The 10-K can appear overly complex at first glance, complete with tables full of data and figures. However, it is so comprehensive that this filing is critical for investors to handle a company's financial position and prospects.Example Files - https://www.kaggle.com/datasets/pranjalverma08/sec-edgar-annual-financial-filings-2021\\n  * Use Regulatory Sources of data to summarize and Q&A\\n* Identify costs & time to train GPT-3 models on a continual daily basis\\n\"}\n",
            "{'prompt': 'aianytyme/gpt-3', 'completion': \"# GPT-3: Language Models are Few-Shot Learners\\n\\n[arXiv link](https://arxiv.org/abs/2005.14165)\\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans.  We discuss broader societal impacts of this finding and of GPT-3 in general.\\n\\n## Contents\\n- [175b_samples.jsonl](175b_samples.jsonl) - Unconditional, unfiltered 2048 token samples from GPT-3 with p=.85, t=1.&#12288;\\n**CONTENT WARNING:** GPT-3 was trained on arbitrary data from the web, so may contain offensive content and language.\\n- [data](data) - Synthetic datasets for word scramble and arithmetic tasks described in the paper.\\n- [dataset_statistics](dataset_statistics) - Statistics for all languages included in the training dataset mix.\\n- [overlap_frequency.md](overlap_frequency.md) - Samples of 13-gram overlaps between our training data and benchmarks, selected by frequency in the training set.\\n- [model-card.md](model-card.md) - GPT-3 Model Card.\\n\\n## How to cite\\n```\\n@article{brown2020language,\\n    title={Language Models are Few-Shot Learners},\\n    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},\\n    year={2020},\\n    eprint={2005.14165},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CL}\\n}\\n```\\n\"}\n",
            "{'prompt': 'aianytyme/sagemaker-huggingface-inference-toolkit', 'completion': '<div style=\"display:flex; text-align:center;\">\\n<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" width=\"100\"/> \\n<img src=\"https://github.com/aws/sagemaker-inference-toolkit/raw/master/branding/icon/sagemaker-banner.png\" width=\"450\"/>\\n</div>\\n\\n\\n\\n\\n# SageMaker Hugging Face Inference Toolkit \\n\\n[![Latest Version](https://img.shields.io/pypi/v/sagemaker_huggingface_inference_toolkit.svg)](https://pypi.python.org/pypi/sagemaker_huggingface_inference_toolkit) [![Supported Python Versions](https://img.shields.io/pypi/pyversions/sagemaker_huggingface_inference_toolkit.svg)](https://pypi.python.org/pypi/sagemaker_huggingface_inference_toolkit) [![Code Style: Black](https://img.shields.io/badge/code_style-black-000000.svg)](https://github.com/python/black)\\n\\n\\nSageMaker Hugging Face Inference Toolkit is an open-source library for serving 🤗 Transformers models on Amazon SageMaker. This library provides default pre-processing, predict and postprocessing for certain 🤗 Transformers models and tasks. It utilizes the [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) for starting up the model server, which is responsible for handling inference requests.\\n\\nFor Training, see [Run training on Amazon SageMaker](https://huggingface.co/docs/sagemaker/train).\\n\\nFor the Dockerfiles used for building SageMaker Hugging Face Containers, see [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers/tree/master/huggingface).\\n\\nFor information on running Hugging Face jobs on Amazon SageMaker, please refer to the [🤗 Transformers documentation](https://huggingface.co/docs/sagemaker).\\n\\nFor notebook examples: [SageMaker Notebook Examples](https://github.com/huggingface/notebooks/tree/master/sagemaker).\\n\\n---\\n## 💻  Getting Started with 🤗 Inference Toolkit\\n\\n_needs to be adjusted -> currently pseudo code_\\n\\n**Install Amazon SageMaker Python SDK**\\n\\n```bash\\npip install sagemaker --upgrade\\n```\\n\\n**Create a Amazon SageMaker endpoint with a trained model.**\\n\\n```python\\nfrom sagemaker.huggingface import HuggingFaceModel\\n\\n# create Hugging Face Model Class\\nhuggingface_model = HuggingFaceModel(\\n    transformers_version=\\'4.6\\',\\n    pytorch_version=\\'1.7\\',\\n    py_version=\\'py36\\',\\n    model_data=\\'s3://my-trained-model/artifacts/model.tar.gz\\',\\n    role=role,\\n)\\n# deploy model to SageMaker Inference\\nhuggingface_model.deploy(initial_instance_count=1,instance_type=\"ml.m5.xlarge\")\\n```\\n\\n\\n**Create a Amazon SageMaker endpoint with a model from the [🤗 Hub](https://huggingface.co/models).**  \\n_note: This is an experimental feature, where the model will be loaded after the endpoint is created. Not all sagemaker features are supported, e.g. MME_\\n```python\\nfrom sagemaker.huggingface import HuggingFaceModel\\n# Hub Model configuration. https://huggingface.co/models\\nhub = {\\n  \\'HF_MODEL_ID\\':\\'distilbert-base-uncased-distilled-squad\\',\\n  \\'HF_TASK\\':\\'question-answering\\'\\n}\\n# create Hugging Face Model Class\\nhuggingface_model = HuggingFaceModel(\\n    transformers_version=\\'4.6\\',\\n    pytorch_version=\\'1.7\\',\\n    py_version=\\'py36\\',\\n    env=hub,\\n    role=role,\\n)\\n# deploy model to SageMaker Inference\\nhuggingface_model.deploy(initial_instance_count=1,instance_type=\"ml.m5.xlarge\")\\n```\\n\\n---\\n\\n## 🛠️ Environment variables\\n\\nThe SageMaker Hugging Face Inference Toolkit implements various additional environment variables to simplify your deployment experience. A full list of environment variables is given below.\\n\\n#### `HF_TASK`\\n\\nThe `HF_TASK` environment variable defines the task for the used 🤗 Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/transformers/main_classes/pipelines.html).\\n\\n```bash\\nHF_TASK=\"question-answering\"\\n```\\n\\n#### `HF_MODEL_ID`\\n\\nThe `HF_MODEL_ID` environment variable defines the model id, which will be automatically loaded from [huggingface.co/models](https://huggingface.co/models) when creating or SageMaker Endpoint. The 🤗 Hub provides +10 000 models all available through this environment variable.\\n\\n```bash\\nHF_MODEL_ID=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n```\\n\\n#### `HF_MODEL_REVISION`\\n\\nThe `HF_MODEL_REVISION` is an extension to `HF_MODEL_ID` and allows you to define/pin a revision of the model to make sure you always load the same model on your SageMaker Endpoint.\\n\\n```bash\\nHF_MODEL_REVISION=\"03b4d196c19d0a73c7e0322684e97db1ec397613\"\\n```\\n\\n#### `HF_API_TOKEN`\\n\\nThe `HF_API_TOKEN` environment variable defines the your Hugging Face authorization token. The `HF_API_TOKEN` is used as a HTTP bearer authorization for remote files, like private models. You can find your token at your [settings page](https://huggingface.co/settings/token).\\n\\n```bash\\nHF_API_TOKEN=\"api_XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\\n```\\n\\n---\\n\\n## 🧑🏻\\u200d💻 User defined code/modules\\n\\nThe Hugging Face Inference Toolkit allows user to override the default methods of the `HuggingFaceHandlerService`. Therefor the need to create a named `code/` with a `inference.py` file in it. You can find an example for it in [sagemaker/17_customer_inference_script](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)\\nFor example:  \\n```bash\\nmodel.tar.gz/\\n|- pytorch_model.bin\\n|- ....\\n|- code/\\n  |- inference.py\\n  |- requirements.txt \\n```\\nIn this example, `pytroch_model.bin` is the model file saved from training, `inference.py` is the custom inference module, and `requirements.txt` is a requirements file to add additional dependencies.\\nThe custom module can override the following methods:  \\n\\n* `model_fn(model_dir)`: overrides the default method for loading the model, the return value `model` will be used in the `predict()` for predicitions. It receives argument the `model_dir`, the path to your unzipped `model.tar.gz`.\\n* `transform_fn(model, data, content_type, accept_type)`: Overrides the default transform function with custom implementation. Customers using this would have to implement `preprocess`, `predict` and `postprocess` steps in the `transform_fn`. **NOTE: This method can\\'t be combined with `input_fn`, `predict_fn` or `output_fn` mentioned below.** \\n* `input_fn(input_data, content_type)`: overrides the default method for prerprocessing, the return value `data` will be used in the `predict()` method for predicitions. The input is `input_data`, the raw body of your request and `content_type`, the content type form the request Header.\\n* `predict_fn(processed_data, model)`: overrides the default method for predictions, the return value `predictions` will be used in the `postprocess()` method. The input is `processed_data`, the result of the `preprocess()` method.\\n* `output_fn(prediction, accept)`: overrides the default method for postprocessing, the return value `result` will be the respond of your request(e.g.`JSON`). The inputs are `predictions`, the result of the `predict()` method and `accept` the return accept type from the HTTP Request, e.g. `application/json`\\n\\n\\n\\n\\n---\\n## 🤝 Contributing\\n\\nPlease read [CONTRIBUTING.md](https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/CONTRIBUTING.md)\\nfor details on our code of conduct, and the process for submitting pull\\nrequests to us.\\n\\n---\\n## 📜  License\\n\\nSageMaker Hugging Face Inference Toolkit is licensed under the Apache 2.0 License.\\n\\n---\\n\\n## 🧑🏻\\u200d💻 Development Environment\\n\\nInstall all test and development packages with \\n\\n```bash\\npip3 install -e \".[test,dev]\"\\n```'}\n",
            "{'prompt': 'aianytyme/NLCA_Question_Generator', 'completion': '# NLCA Question Generator\\n\\nThe Natural Language Cognitive Architecture (NLCA) requires the ability to generate questions for several purposes. First, generating questions is one of the primary ways it \"thinks\". Curiosity is merely unspoken internal questions. The ability to spontaneously generate questions leads to the ability to seek answers, and thus accumulate more information. Just asking questions and seeking answers could be one path to AGI.\\n\\nGenerating good questions is the root of curiosity and learning. By asking follow-up questions, and seeking answers, you can create a positive-feedback loop which results in greater understanding. Furthermore, it just makes for a good conversational companion. For instance, you could have a chatbot that only asks questions and still have a tremendously valuable tool.\\n\\n## Scheme\\n\\nYou start with a context. In this case, a context is a scenario, story, article, or dialog. This context then has questions associated with it. This forms the training data for the finetuned model. Here\\'s an example context:\\n\\n- Does anyone else\\'s stomach gets upset when anxious? My stomach always begins to ache when anxious. At times I have to excrete too. Sometimes it is painful.\\n\\nAnd here\\'s an example set of questions:\\n\\n- Are you constantly anxious?\\n- What happens when you\\'re anxious?\\n- How often does this happen?\\n- Does anything else trigger your anxiety?\\n\\nHere\\'s what the same thing looks like as training data:\\n\\n```json\\n{\"prompt\": \"Does anyone else\\'s stomach gets upset when anxious ?\\\\nMy stomach always begins to ache when anxious . At times I have to excrete too . Sometimes it is painful .\\\\nQUESTIONS:\", \"completion\": \"Are you constantly anxious?\\\\nWhat happens when you\\'re anxious?\\\\nHow often does this happen?\\\\nDoes anything else trigger your anxiety?\"}\\n```\\n\\n## Usage Quickstart\\n\\n### Training\\n\\nI wrote a file called `finetune.py` that contains a few functions to help speed things along. All you need to do is run a few CLI commands to get started. Make sure you create `openaiapikey.txt` first, and put your API key in it.\\n\\n```python\\nfrom finetune import *\\nfile_upload(\\'fine-tune\\', \\'questions.jsonl\\')\\nfinetune_model(\\'<fileid returned from file_upload command-\\')\\nfinetune_list()\\n```\\n\\nThe training data is ready to go as `questions.jsonl`!\\n\\n### Completion\\n\\nI wrote a quick demo script called `test_finetune.py`. You\\'ll need to populate it with your own finetuned model name. You can see it with the above `finetune_list()` function. Here\\'s an example of the performance:\\n\\nContext:\\n\\n- So guys I have this crush on this boy and he\\'s so dumb but I love him. His name is Roger and he\\'s the quarterback of the team.\\n\\nQuestions:\\n\\n- What is the narrator\\'s name?\\n- How old is the narrator?\\n- What team does the narrator play on?\\n- What is the narrator\\'s relationship to Roger?\\n- Does the narrator have a crush on Roger?\\n- What is Roger\\'s last name?\\n- Where does Roger play on the team?\\n- What is Roger\\'s position on the team?\\n- Does the narrator have any other friends on the team?\\n- Does the narrator have any other crushes besides Roger?\\n- What is the name of the school that the narrator attends?\\n- What is the name of the team that Roger plays for and what position does he play?\\n- Does the narrator have any brothers or sisters?\\n- Does the narrator go to a public or private school?\\n- What is the name of the city that the narrator lives in or where they live?\\n- What sports does the narrator play?\\n- Does the narrator have any brothers or sisters that play on the team?\\n\\nThis list is a bit ridiculous but you can see how it really took the idea of asking questions and ran with it. These questions can be used for conversational purposes, or for searching a database with information about the narrator. For instance, imagine that you have a personal assistant that has recorded all previous conversations with you. These kinds of questions can be used internally for the information assistant to recall the correct details and hold a realistic conversations. Alternatively, these questions can be used to have a real-time conversation.\\n\\n\\n## Explanation of Everything\\n\\n### Contexts\\n\\nI scraped together the contents of the `contexts` folder from various data sources, mostly from Kaggle. Another good source was https://github.com/Guzpenha/MANtIS. Check in the `data_prep_scripts` for some of the data cleaning scripts I wrote. The idea was to pare down the contexts to bare minimum, so as to remove some framing, comments, and other noise. I also included a variety of types of contexts so that the AGI would be familiar with many types of situations. There are medical cases, reddit and stack exchange posts, movie dialog, and news snippets. This combination means that the finetuned model will be excellent at generating questions for almost any scenario, whether it\\'s one person asking for help, talking to multiple people, or reacting to news.\\n\\nCheck in the `contexts` folder for a list of all contexts. I only used 1200 of them for the training set, but I have more than 50,000 available for future use.\\n\\nTypes of contexts used:\\n1. Reddit posts from many different subreddits\\n2. Stack Exchange posts from several domains\\n3. News articles, mostly British\\n4. Medical notes\\n5. Dialog from movies\\n\\n### Questions\\n\\nI found that GPT-3 is great at asking questions if you give it a lot of power. I used `DAVINCI-INSTRUCT-BETA` and gave it the simple command of `Write a list of the most important and salient questions an observer would ask about the following passage:`. This prompt generated the best results by far. For instance, GPT-3 was able to read between the lines on more emotional contexts or infer the medical condition on doctors notes. GPT-3 implicitly demonstrated that it has strong knowledge about the world - far more than any one person. I learned a lot just by looking at the questions it asked!\\n\\nI set the temperature pretty high on GPT-3 so that it would be as creative as possible with the questions. I also ran it a second time to ensure that each context had at least 4 questions asked. Plus, I wrote a script that would remove any lines from the questions that did not end in a question mark. \\n\\nCheck in the `questions` folder for all generated questions. I spent about $120 of DAVINCI tokens to generate the datasets.\\n\\n### Training Data\\n\\nFinally, I put it all together with `format_question_training.py`. This creates a JSONL file with all the questions and their associated contexts. It also adds `QUESTIONS:` to the end of the context becuase the model will need to know when to switch to questions. \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write data in JSONL file**"
      ],
      "metadata": {
        "id": "-RmnH0pY1a1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(PATH+'data/test.jsonl','w') as writer:\n",
        "  writer.write_all(jsons)"
      ],
      "metadata": {
        "id": "OuHzCGIDyuAe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune the OpenAI model"
      ],
      "metadata": {
        "id": "ms-iEzHF1-Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a new model using the JSONL file**"
      ],
      "metadata": {
        "id": "7ZEM4ld42J3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH\n",
        "!openai api fine_tunes.create -t $PATH/data/test.jsonl -m ada --suffix \"gpt3-for-capital-markets\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw554gxL2VrY",
        "outputId": "eb492151-c8f3-4d49-cb74-2e7409b3d070"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Projects/GPT-3/\n",
            "Upload progress: 100% 20.3k/20.3k [00:00<00:00, 21.5Mit/s]\n",
            "Uploaded file from /content/drive/MyDrive/Projects/GPT-3//data/test.jsonl: file-zs2dFKBcEArAiFCUm6MHrCAU\n",
            "Created fine-tune: ft-eeVEzN2ToyLLh4iUYrYTlArf\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-05-02 17:33:02] Created fine-tune: ft-eeVEzN2ToyLLh4iUYrYTlArf\n",
            "[2022-05-02 17:33:09] Fine-tune costs $0.01\n",
            "[2022-05-02 17:33:09] Fine-tune enqueued. Queue number: 0\n",
            "[2022-05-02 17:33:12] Fine-tune started\n",
            "[2022-05-02 17:33:27] Completed epoch 1/4\n",
            "[2022-05-02 17:33:28] Completed epoch 2/4\n",
            "[2022-05-02 17:33:29] Completed epoch 3/4\n",
            "[2022-05-02 17:33:30] Completed epoch 4/4\n",
            "[2022-05-02 17:33:47] Uploaded model: ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-05-02-17-33-45\n",
            "[2022-05-02 17:33:50] Uploaded result file: file-d3WC3LTCIfJnaD1TddbR5289\n",
            "[2022-05-02 17:33:50] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-05-02-17-33-45 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the new model**"
      ],
      "metadata": {
        "id": "jLVyQ9Qc7MlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api completions.create -m ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-29-13-36-35 -p \"The Hugging Face Inference Toolkit allows\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KsjPUa13cq8",
        "outputId": "6b3d28de-1d46-48cc-f0a5-8db88f5e8f09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Hugging Face Inference Toolkit allows you to apply machine learning to facial expressions and ask labelling questions on them."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Delete the model(s)**"
      ],
      "metadata": {
        "id": "SQzNzOBI1kIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api models.delete -i ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-29-13-36-35\n",
        "#!openai api models.delete -i ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-25-02-54-08\n",
        "#!openai api models.delete -i ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-25-11-47-03\n",
        "#!openai api models.delete -i ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-26-02-33-18\n",
        "#!openai api models.delete -i ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-05-02-17-33-45\n"
      ],
      "metadata": {
        "id": "lWPof4eX7gG8",
        "outputId": "c9716d3d-68fd-4cc7-b699-6cc069addecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"deleted\": true,\n",
            "  \"id\": \"ada:ft-the-orange-pencil:gpt3-for-capital-markets-2022-04-29-13-36-35\",\n",
            "  \"object\": \"model\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}